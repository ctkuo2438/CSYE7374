{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2ceeca-2847-4d61-a310-ec08d2e18091",
   "metadata": {},
   "source": [
    "# Assignment 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a553c6-c91d-4835-943f-86099b1cd536",
   "metadata": {},
   "source": [
    "## 1. Background and Motivation\n",
    "Pretraining foundation models requires large-scale, diverse, and high-quality datasets. Raw text data often contains duplicates, noise, or formatting issues that can negatively impact model learning. Effective preprocessing‚Äîincluding cleaning, normalization, tokenization, and batching‚Äîis crucial for training eÔ¨Äicient, reliable models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5aaac-d13f-4ede-8b5f-8f89f9ba7ca7",
   "metadata": {},
   "source": [
    "## 2. Learning Objectives\n",
    "\n",
    "By completing this assignment, students will be able to:\n",
    "\n",
    "1. Identify and access **publicly available large-scale text datasets** suitable for pretraining.\n",
    "2. Implement **data cleaning and normalization pipelines**, including deduplication and low-quality content removal.\n",
    "3. Apply **tokenization strategies** suitable for transformer-based foundation models.\n",
    "4. Develop **custom data loaders** in PyTorch or TensorFlow for eÔ¨Äicient batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22165dfb-7277-4d82-8e71-2cffdc367cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhenting/miniconda3/envs/7374_LLM/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# libs\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import re\n",
    "import random\n",
    "from torch.utils.data import IterableDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323dfa14-b57d-468f-8b77-de25fe8aa75f",
   "metadata": {},
   "source": [
    "## 3. Dataset Selection for Foundation Model Pre-Training\n",
    "\n",
    "I selected three diverse datasets to meet the multi-domain requirement (encyclopedic, news, general web text) for pre-training data collection.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Overview Table\n",
    "\n",
    "| Feature | CC-News (Original) | Wikipedia (Wikimedia) | OpenWebText |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Domain** | News | Encyclopedic | General Web |\n",
    "| **HuggingFace Path** | [`cc_news`](https://huggingface.co/datasets/cc_news) | [`wikimedia/wikipedia`](https://huggingface.co/datasets/wikimedia/wikipedia) | [`Skylion007/openwebtext`](https://huggingface.co/datasets/Skylion007/openwebtext) |\n",
    "| **Temporal Range** | Jan 2017 ‚Äì Dec 2019 | November 2023 dump | 2019 (Reddit-sourced) |\n",
    "| **Volume** | ~708k English articles | 6.4M English articles | 8M+ documents (24.2 GB) |\n",
    "| **Pre-processing** | **Raw (Requires Deduplication)** | Markdown/references stripped | Deduplicated, English-filtered |\n",
    "| **Status** | Ready for cleaning | Ready for normalization | Ready for normalization |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Processing Requirements\n",
    "\n",
    "#### 1. CC-News (Original) ‚Äî News Domain\n",
    "* **Description:** A dataset containing news articles from news sites all over the world, spanning Jan 2017 to Dec 2019. \n",
    "* **Implementation Steps:**\n",
    "    * [ ] **Deduplication (Required)**\n",
    "    * [x] Text normalization (lowercase, whitespace)\n",
    "    * [x] Short document filtering (< 50 words)\n",
    "    * [x] Symbol/noise removal\n",
    "\n",
    "#### 2. Wikipedia (Wikimedia) ‚Äî Encyclopedic Domain\n",
    "* **Description:** Cleaned English Wikipedia articles with markdown and reference sections already removed.\n",
    "* **Implementation Steps:**\n",
    "    * [x] Text normalization (lowercase, whitespace)\n",
    "    * [x] Short document filtering (< 50 words)\n",
    "    * [x] Symbol/noise removal\n",
    "\n",
    "#### 3. OpenWebText ‚Äî General Web Domain\n",
    "* **Description:** Open-source replication of OpenAI's WebText dataset, sourced from Reddit-upvoted web pages.\n",
    "* **Implementation Steps:**\n",
    "    * [x] Text normalization (lowercase, whitespace)\n",
    "    * [x] Short document filtering (< 50 words)\n",
    "    * [x] Symbol/noise removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f559c497-b6a2-46fd-8107-d3c8ce52bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CC-News...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "News: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:12<00:00, 7716.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wikipedia: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:18<00:00, 5525.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenWebText...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WebText: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:15<00:00, 6548.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 1.13 GB\n",
      "Data saved to /Users/zhenting/7374_LLM/raw_dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_file = \"/Users/zhenting/7374_LLM/Assignment_01/raw_dataset.jsonl\"\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Sample sizes\n",
    "news_samples = 100000\n",
    "wiki_samples = 100000\n",
    "web_samples = 100000\n",
    "\n",
    "# Helper function to append to file immediately to save RAM\n",
    "def stream_to_file(dataset, num_samples, domain_name, pbar_desc):\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        iterator = dataset[\"train\"].take(num_samples)\n",
    "        for ex in tqdm(iterator, total=num_samples, desc=pbar_desc):\n",
    "            record = {\n",
    "                \"text\": ex[\"text\"],\n",
    "                \"domain\": domain_name\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 1. Collect News\n",
    "print(\"Loading CC-News...\")\n",
    "cc_news = load_dataset(\"cc_news\", streaming=True)\n",
    "stream_to_file(cc_news, news_samples, \"news\", \"News\")\n",
    "\n",
    "# 2. Collect Wikipedia\n",
    "print(\"Loading Wikipedia...\")\n",
    "wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", streaming=True)\n",
    "stream_to_file(wiki, wiki_samples, \"encyclopedic\", \"Wikipedia\")\n",
    "\n",
    "# 3. Collect OpenWebText\n",
    "print(\"Loading OpenWebText...\")\n",
    "openwebtext = load_dataset(\"Skylion007/openwebtext\", streaming=True)\n",
    "stream_to_file(openwebtext, web_samples, \"web\", \"WebText\")\n",
    "\n",
    "total_bytes = os.path.getsize(output_file)\n",
    "total_gb = total_bytes / (1024 ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e9421-2582-4139-a067-817152ba4011",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Requirements\n",
    "\n",
    "### 4.1 Cleaning\n",
    "* Remove duplicate documents.\n",
    "* Normalize text: lowercase, remove extra whitespace, strip irrelevant symbols.\n",
    "* Remove low-quality or very short documents (e.g., fewer than 50 words).\n",
    "* Optionally remove HTML tags, markdown, or reference markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52402575-68cb-44e3-bd47-ef8a808c94cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing pipeline on /Users/zhenting/7374_LLM/raw_dataset.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [00:36<00:00, 8127.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing Report ===\n",
      "Original Docs: 300000\n",
      "Duplicates Removed: 16358\n",
      "Short Docs Removed (<50 words): 9921\n",
      "Final Docs Kept: 273721\n",
      "Final Dataset Size: 1.10 GB\n",
      "Cleaned data saved to: /Users/zhenting/7374_LLM/clean_dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/Users/zhenting/7374_LLM/Assignment_01/raw_dataset.jsonl\"\n",
    "output_file = \"/Users/zhenting/7374_LLM/Assignment_01/clean_dataset.jsonl\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: {input_file} not found!\")\n",
    "else:\n",
    "    stats = {\n",
    "        \"total_processed\": 0,\n",
    "        \"duplicates_removed\": 0,\n",
    "        \"short_docs_removed\": 0,\n",
    "        \"kept\": 0\n",
    "    }\n",
    "\n",
    "    seen_hashes = set()\n",
    "    print(f\"Starting preprocessing pipeline on {input_file}...\")\n",
    "    total_lines = sum(1 for _ in open(input_file, encoding=\"utf-8\"))\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        \n",
    "        for line in tqdm(fin, total=total_lines, desc=\"Cleaning\"):\n",
    "            stats[\"total_processed\"] += 1\n",
    "            \n",
    "            try:\n",
    "                doc = json.loads(line)\n",
    "                text = doc.get(\"text\", \"\")\n",
    "                \n",
    "                # Normalize text: lowercase, remove extra whitespace\n",
    "                text_clean = re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "                \n",
    "                # Remove duplicate documents\n",
    "                # Ex: 'Hello' -> 8b1a9953c4611296a827abf8c47804d7, easier to compare\n",
    "                text_hash = hashlib.md5(text_clean.encode(\"utf-8\")).hexdigest()\n",
    "                if text_hash in seen_hashes:\n",
    "                    stats[\"duplicates_removed\"] += 1\n",
    "                    continue\n",
    "                seen_hashes.add(text_hash)\n",
    "                \n",
    "                # Remove low-quality or very short documents (< 50 words)\n",
    "                word_count = len(text_clean.split())\n",
    "                if word_count < 50:\n",
    "                    stats[\"short_docs_removed\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                doc[\"text\"] = text_clean\n",
    "                fout.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "                stats[\"kept\"] += 1\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    clean_size_gb = os.path.getsize(output_file) / (1024 ** 3)\n",
    "    print(f\"Original Docs: {stats['total_processed']}\")\n",
    "    print(f\"Duplicates Removed: {stats['duplicates_removed']}\")\n",
    "    print(f\"Short Docs Removed (<50 words): {stats['short_docs_removed']}\")\n",
    "    print(f\"Final Docs Kept: {stats['kept']}\")\n",
    "    print(f\"Final Dataset Size: {clean_size_gb:.2f} GB\")\n",
    "    print(f\"Cleaned data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c37cd0-9875-466b-a8b8-5e625e2b5237",
   "metadata": {},
   "source": [
    "## 4.2 Tokenization\n",
    "\n",
    "* Use a transformer-compatible tokenizer (Hugging Face AutoTokenizer or similar).\n",
    "* Support BPE, WordPiece, or GPT-style tokenization.\n",
    "* Handle sequences longer than the model‚Äôs maximum block size via chunking.\n",
    "* Maintain tokenized sequences in an eÔ¨Äicient data structure (list of lists, arrays, or tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4639f507-fd95-4a4d-8e64-c38df21996df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: gpt2...\n",
      "Tokenizing with gpt2 (Block size: 1024)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:   0%|                                              | 0/273721 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1123 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 79668/273721 [00:44<13:21, 242.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50004\n",
      "tensor([ 8117,   338,   257,  ..., 16667, 28841,   365])\n",
      "tensor([[ 8117,   338,   257,  ...,    13,  2008, 12537],\n",
      "        [  299,    88, 21101,  ...,    85,  4763, 47735],\n",
      "        [21421,    13,   299,  ...,   717,  5545,  3652],\n",
      "        ...,\n",
      "        [40138,   410, 40138,  ..., 45630,   272, 14549],\n",
      "        [19322,   784, 44873,  ...,  3104,   784,   556],\n",
      "        [ 2516,   286,   277,  ..., 16667, 28841,   365]])\n",
      "Saved shard 0 to /Users/zhenting/7374_LLM/tokenized_data/shard_0.pt: shape torch.Size([50004, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 122192/273721 [01:34<02:39, 949.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([   11, 45630,   272,  ..., 23645,  3835,  7097])\n",
      "tensor([[   11, 45630,   272,  ...,  2272,   329,   257],\n",
      "        [ 4996,   286, 12420,  ..., 10389,  5823,   423],\n",
      "        [ 4054,  2233,   284,  ..., 20835,     1,   416],\n",
      "        ...,\n",
      "        [ 1989,   319,   262,  ...,   318,  1900,   355],\n",
      "        [50162,   272, 12480,  ...,   262,  3814,   810],\n",
      "        [  262,  4618,   550,  ..., 23645,  3835,  7097]])\n",
      "Saved shard 1 to /Users/zhenting/7374_LLM/tokenized_data/shard_1.pt: shape torch.Size([50000, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 172754/273721 [02:23<01:25, 1175.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([ 6117, 23185,  1872,  ...,  1097, 33280,   936])\n",
      "tensor([[ 6117, 23185,  1872,  ..., 30997,    13,   355],\n",
      "        [  530,   286,   262,  ...,  1872,   289,  3536],\n",
      "        [  286, 48569,    77,  ...,  5663,   284,   262],\n",
      "        ...,\n",
      "        [43886,   290,   262,  ...,   284,  3714,   572],\n",
      "        [38306,  1911, 13304,  ...,   287,   262,  4320],\n",
      "        [   11,   951,   292,  ...,  1097, 33280,   936]])\n",
      "Saved shard 2 to /Users/zhenting/7374_LLM/tokenized_data/shard_2.pt: shape torch.Size([50000, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 218308/273721 [03:14<00:58, 952.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([39818,  1621,  2614,  ...,   257,  2408,   290])\n",
      "tensor([[39818,  1621,  2614,  ..., 41899, 28936, 20840],\n",
      "        [  262,  1989,    11,  ...,  7867,   416,   607],\n",
      "        [ 2802,    11,  2855,  ...,   727,   285,   446],\n",
      "        ...,\n",
      "        [   13,   339,   373,  ...,  4695,    11,  6134],\n",
      "        [   11, 19435,    11,  ...,  4441, 15962, 30162],\n",
      "        [  329,  3511,    13,  ...,   257,  2408,   290]])\n",
      "Saved shard 3 to /Users/zhenting/7374_LLM/tokenized_data/shard_3.pt: shape torch.Size([50000, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 264308/273721 [04:03<00:09, 1037.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50001\n",
      "tensor([6283,  640,  329,  ...,   11,  286, 1781])\n",
      "tensor([[ 6283,   640,   329,  ...,   447,   247,    82],\n",
      "        [ 4925,   286,  1321,  ...,   437, 21361,    31],\n",
      "        [12398,    78,    13,  ...,   479,  2002,    83],\n",
      "        ...,\n",
      "        [14169,  1636,    13,  ...,   393, 21628,    69],\n",
      "        [   11,   484,  1422,  ...,   878,   465,  6626],\n",
      "        [  351,   474,  1697,  ...,    11,   286,  1781]])\n",
      "Saved shard 4 to /Users/zhenting/7374_LLM/tokenized_data/shard_4.pt: shape torch.Size([50001, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 273721/273721 [04:15<00:00, 1072.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10208\n",
      "tensor([   11,   345,   743,  ...,   339, 13339,   423])\n",
      "tensor([[   11,   345,   743,  ...,   373, 40316, 30982],\n",
      "        [  284,   262,  2642,  ...,  4706,  3636,   470],\n",
      "        [  910,   644,  3022,  ...,  1176,   379,   281],\n",
      "        ...,\n",
      "        [ 1280,   262,  5739,  ...,   284, 23982,   884],\n",
      "        [  355, 12574,  2049,  ...,  5526,  2370,   422],\n",
      "        [ 3770, 29136,  8636,  ...,   339, 13339,   423]])\n",
      "Saved shard 5 to /Users/zhenting/7374_LLM/tokenized_data/shard_5.pt: shape torch.Size([10208, 1024])\n",
      "\n",
      "=== Preprocessing Complete ===\n",
      "Check output directory: /Users/zhenting/7374_LLM/tokenized_data/\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/Users/zhenting/7374_LLM/Assignment_01/clean_dataset.jsonl\"\n",
    "output_dir = \"/Users/zhenting/7374_LLM/Assignment_01/tokenized_data\"\n",
    "model_name = \"gpt2\" # Byte-Level\n",
    "block_size = 1024\n",
    "shard_size = 50000\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Tokenizer (Transformer-compatible)\n",
    "print(f\"Loading tokenizer: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# GPT-2 has no padding token, since sentence need to be the same length, so we use eos_token for padding\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "def save_shard(tokens_list, shard_idx):\n",
    "    \"\"\"\n",
    "    4.2 Requirement: Maintain tokenized sequences in an efficient data structure (tensors).\n",
    "    \"\"\"\n",
    "    if not tokens_list:\n",
    "        return []\n",
    "    \n",
    "    # list -> tensor for GPU\n",
    "    tensor_data = torch.tensor(tokens_list, dtype=torch.long)\n",
    "    \n",
    "    # 4.2 Requirement: Handle sequences longer than block size via chunking.\n",
    "    num_blocks = len(tensor_data) // block_size # 1024\n",
    "    \n",
    "    if num_blocks > 0:\n",
    "        cutoff = num_blocks * block_size\n",
    "        tensor_data = tensor_data[:cutoff] # Slicing\n",
    "        \n",
    "        # Reshape Ex: [2048] -> [2, 1024], two data, each 1024\n",
    "        tensor_data = tensor_data.view(-1, block_size)\n",
    "        \n",
    "        save_path = os.path.join(output_dir, f\"shard_{shard_idx}.pt\")\n",
    "        torch.save(tensor_data, save_path)\n",
    "        print(f\"Saved shard {shard_idx} to {save_path}: shape {tensor_data.shape}\") # (50000 row, 1024 col token)\n",
    "        \n",
    "        remaining = tokens_list[cutoff:]\n",
    "        return remaining\n",
    "    \n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "print(f\"Tokenizing with {model_name} (Block size: {block_size})...\")\n",
    "all_tokens = []\n",
    "shard_count = 0\n",
    "doc_count = 0\n",
    "eos_id = tokenizer.eos_token_id\n",
    "\n",
    "total_lines = sum(1 for _ in open(input_file, encoding=\"utf-8\"))\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=total_lines, desc=\"Tokenizing\"):\n",
    "        try:\n",
    "            doc = json.loads(line)\n",
    "            text = doc[\"text\"]\n",
    "            \n",
    "            # Tokenization\n",
    "            tokens = tokenizer.encode(text) + [eos_id]\n",
    "            all_tokens.extend(tokens)\n",
    "            doc_count += 1\n",
    "            \n",
    "            # Memory Management\n",
    "            if len(all_tokens) > shard_size * block_size:\n",
    "                all_tokens = save_shard(all_tokens, shard_count)\n",
    "                shard_count += 1\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "if all_tokens:\n",
    "    save_shard(all_tokens, shard_count)\n",
    "\n",
    "print(\"\\n=== Preprocessing Complete ===\")\n",
    "print(f\"Check output directory: {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe3260-3011-4e4f-98b8-8d9533a6eff6",
   "metadata": {},
   "source": [
    "## 4.3 Custom Data Loader\n",
    "\n",
    "* Implement a custom data loader in PyTorch or TensorFlow:\n",
    "    - PyTorch: torch.utils.data.Dataset and DataLoader\n",
    "    - TensorFlow: tf.data.Dataset pipeline\n",
    "* Support batching and shuffling.\n",
    "* Handle variable-length sequences with padding or truncation if needed.\n",
    "* Enable iterable streaming for large datasets to avoid memory bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237b96bd-fba4-4928-b373-3dbb9ca2d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard: shard_1.pt\n",
      "Batch 0 shape: torch.Size([8, 1024])\n",
      "Batch 1 shape: torch.Size([8, 1024])\n",
      "Batch 2 shape: torch.Size([8, 1024])\n",
      "Data Loader test passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k_/401hsb7n19s13514r80fb9h80000gn/T/ipykernel_52484/1920814641.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path) # shape: (N, 1024)\n"
     ]
    }
   ],
   "source": [
    "class GPTDataset(IterableDataset):\n",
    "    def __init__(self, data_dir, shuffle=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.shuffle = shuffle\n",
    "        self.shard_paths = [\n",
    "            os.path.join(data_dir, f) \n",
    "            for f in os.listdir(data_dir) \n",
    "            if f.startswith(\"shard_\") and f.endswith(\".pt\")\n",
    "        ]\n",
    "        self.shard_paths.sort()\n",
    "\n",
    "    def load_shard(self, path):\n",
    "        print(f\"Loading shard: {os.path.basename(path)}\")\n",
    "        data = torch.load(path) # shape: (N, 1024)\n",
    "        return data\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Core logic of the IterableDataset:\n",
    "        1. Determine the reading order of Shards (Shard-level shuffling).\n",
    "        2. Load one Shard into memory at a time.\n",
    "        3. Yield each row (Block) from the loaded Shard individually.\n",
    "        \"\"\"\n",
    "        # Shard Level Shuffle: Randomize the order of shard files to be processed.\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        shard_indices = list(range(len(self.shard_paths)))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(shard_indices)\n",
    "        \n",
    "        for idx in shard_indices:\n",
    "            shard_path = self.shard_paths[idx]\n",
    "            \n",
    "            # Load shard data -> RAM at this point\n",
    "            shard_data = self.load_shard(shard_path)\n",
    "            num_samples = shard_data.shape[0]\n",
    "            \n",
    "            # Sample Level Shuffle\n",
    "            indices = list(range(num_samples))\n",
    "            if self.shuffle:\n",
    "                random.shuffle(indices)\n",
    "            \n",
    "            # Yield samples (one by one (row))\n",
    "            for i in indices:\n",
    "                yield shard_data[i] # (1024,)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8 # 8 rows in each training process\n",
    "data_dir = \"/Users/zhenting/7374_LLM/Assignment_01/tokenized_data\"\n",
    "dataset = GPTDataset(data_dir, shuffle=True)\n",
    "\n",
    "# instance DataLoader using Pytorch, packaging into batch\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(f\"Batch {i} shape: {batch.shape}\")\n",
    "    # torch.Size([8, 1024])\n",
    "    if i >= 2: # top 3 batch \n",
    "        break\n",
    "\n",
    "print(\"Data Loader test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd307057-0a94-4250-9894-16523703b1d9",
   "metadata": {},
   "source": [
    "### sample batches of processed/tokenized data (e.g., first 5‚Äì10 blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "564a6f1d-8b84-4059-949f-83c02b8e20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Shape: torch.Size([10, 1024])\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/Users/zhenting/7374_LLM/Assignment_01\"\n",
    "shard_filename = \"tokenized_data/shard_0.pt\"\n",
    "output_filename = \"sample_batch.pt\"\n",
    "load_path = os.path.join(base_dir, shard_filename)\n",
    "save_path = os.path.join(base_dir, output_filename)\n",
    "\n",
    "if os.path.exists(load_path):\n",
    "    full_data = torch.load(load_path, weights_only=True)\n",
    "    sample_data = full_data[:10]\n",
    "    torch.save(sample_data, save_path)\n",
    "    print(f\"Sample Shape: {sample_data.shape}\")\n",
    "else:\n",
    "    print(f\"Error: {load_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3219f8a-6522-404b-802d-faffe6ae42b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([10, 1024])\n",
      "tensor([[ 8117,   338,   257,  ...,    13,  2008, 12537],\n",
      "        [  299,    88, 21101,  ...,    85,  4763, 47735],\n",
      "        [21421,    13,   299,  ...,   717,  5545,  3652],\n",
      "        [ 3702,   500,   338,  ...,   284,   513,    11],\n",
      "        [  830,  2444,   290,  ..., 13289,  7898,   262]])\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/zhenting/7374_LLM/Assignment_01/sample_batch.pt\"\n",
    "data = torch.load(file_path, weights_only=True)\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a9253-bb71-4239-820a-00117c5f7107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285ed7b-5956-4376-ac05-f91f62a3a420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "7374_LLM",
   "language": "python",
   "name": "7374_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
