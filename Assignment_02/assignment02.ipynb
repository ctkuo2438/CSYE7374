{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492e2409-ed68-49f4-bc1b-591ce1061e0c",
   "metadata": {},
   "source": [
    "# Building a Small-Scale Foundation Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf10ba-517a-450b-8a09-01fee0911e70",
   "metadata": {},
   "source": [
    "## 1 Background and Motivation\n",
    "\n",
    "Training a small-scale transformer from scratch helps students understand the core architecture and dynamics of foundation models. By implementing a mini-GPT and performing **next-token prediction**, students learn how tokenization, model architecture, and hyperparameters impact learning and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70035e4d-d919-45cb-9c16-8bad68243e7e",
   "metadata": {},
   "source": [
    "## 2 Learning Objectives\n",
    "\n",
    "1. Implement a transformer-based language model using PyTorch.\n",
    "2. Train a model from scratch on preprocessed datasets for next-token prediction.\n",
    "3. Track training metrics such as loss and **perplexity**.\n",
    "4. Experiment with hyperparameters (learning rate, batch size, sequence length, number of layers).\n",
    "5. Save and load model checkpoints.\n",
    "6. Visualize training dynamics and interpret results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4df54b-7af2-43ad-a1d1-559c52fe430a",
   "metadata": {},
   "source": [
    "## 3 Model Requirements\n",
    "### Mini-GPT\n",
    "\n",
    "* 1–2 transformer layers\n",
    "* Embedding dimension: 64–256\n",
    "* Multi-head attention: 2–4 heads\n",
    "* Positional embedding\n",
    "* Layer normalization + activation function\n",
    "* Output logits for next-token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d105234-70fe-4e55-b7b6-d4ae323622c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import os, math, random, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304502c-ba3c-4fc9-bbd2-bcbe8a2667c3",
   "metadata": {},
   "source": [
    "### Step 1: Data Loader\n",
    "\n",
    "* From assignment1: each shard_*.pt shape is (50000, 1024), 50000 rows and each row contain 1024 tokens\n",
    "* For assignment2: block_size require 128 tokens\n",
    "\n",
    "```\n",
    "shard_0.pt (50000, 1024)\n",
    "  → reshape → 51,200,000 tokens\n",
    "  → cut into 129-token \n",
    "  → shuffle\n",
    "  → yield (x, y) to DataLoader\n",
    "\n",
    "x = [A, B, C, ... 128]   # input\n",
    "y = [B, C, D, ... 128]   # target（next-token prediction）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a99a07-6da8-464b-b966-ab809be1bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(IterableDataset):\n",
    "    def __init__(self, data_dir, block_size=128, shuffle=True):\n",
    "        self.block_size  = block_size\n",
    "        self.shuffle     = shuffle\n",
    "        self.shard_paths = sorted(\n",
    "            [os.path.join(data_dir, f) for f in os.listdir(data_dir)\n",
    "             if f.startswith('shard_') and f.endswith('.pt')]\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.shard_paths)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "        for idx in indices:\n",
    "            shard = torch.load(self.shard_paths[idx], weights_only=True)\n",
    "            flat  = shard.reshape(-1) # (50000, 1024) → (51_200_000,)\n",
    "            # chunk = 129 because each training sample needs:\n",
    "            #   x = 128 tokens (input)\n",
    "            #   y = 128 tokens (target, shifted right by 1)\n",
    "            chunk = self.block_size + 1\n",
    "            n_blocks = len(flat) // chunk\n",
    "\n",
    "            # chunks of 129 tokens: flat[0:129], flat[129:258], flat[258:387], ...\n",
    "            rows = [flat[i*chunk : (i+1)*chunk] for i in range(n_blocks)]\n",
    "            \n",
    "            if self.shuffle:\n",
    "                random.shuffle(rows)\n",
    "                \n",
    "            for row in rows:\n",
    "                # Split each 129-token chunk into an (x, y) training pair\n",
    "                # x = row[0:128]  → input tokens\n",
    "                # y = row[1:129]  → target tokens (next-token for each position)\n",
    "                yield row[:-1], row[1:] # x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc37acc-b812-460c-afaa-aefc41e32cb2",
   "metadata": {},
   "source": [
    "### Step2: model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bfb1c0-f2f7-46c1-88a5-b2f7351c7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head # 4 heads\n",
    "        self.n_embd = n_embd # dimension 128\n",
    "        self.head_dim = n_embd // n_head # 128 // 4 = 32, each head deal with 32D\n",
    "        \n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd) # Q, K, V, each 128D\n",
    "        # att = softmax(Q · K^T) \n",
    "        # calculate the attention scores to determine which tokens are most relevant to the current query\n",
    "        # out = att · V\n",
    "        # Compute the weighted sum of the values to aggregate information from the most relevant tokens\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd) # fuses information\n",
    "        self.drop = nn.Dropout(dropout) # prevent overfitting\n",
    "        \n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        # (Batch Size, Num Heads, Seq Length, Seq Length)\n",
    "        self.register_buffer('mask', mask.view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # # B = batch size, T = sequence Length (128 tokens), C = n_embd (128)\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # (B,T,128)\n",
    "        # # (B, T, 128) → (B, T, 4, 32) → (B, 4, T, 32)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scale = 1.0 / math.sqrt(self.head_dim) # avoid gradient vanishing\n",
    "        att = (q @ k.transpose(-2, -1)) * scale # get attention score\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf')) # masking\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.drop(att)\n",
    "        \n",
    "        out = att @ v # (B, 4, T, 32), weighted sum of values\n",
    "        # (B, 4, T, 32) → (B, T, 4, 32) → (B, T, 128)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.c_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf6f21f-0a87-4a7d-9200-f328207355d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), # 128 to 512\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = SelfAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffn = FFN(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x)) # Residual Connections, do LayerNorm first\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e7ffa7e-24fb-40b1-9448-45560b4b1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd) # token embedding, each word 128D\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd) # position encoding\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embd, n_head, block_size, dropout)\n",
    "            for _ in range(n_layer)\n",
    "        ]) # stacked transformer layers\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        x = self.drop(self.tok_emb(idx) + self.pos_emb(pos)) # add positional information\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x) # score next word\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abf642-a9ba-443f-8aaa-b618858683d2",
   "metadata": {},
   "source": [
    "### Step 3: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047b73de-fb48-4656-9f1f-be547f41a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, cfg, data_dir, block_size, batch_size, lr,\n",
    "          n_epochs, max_steps = 2000, ckpt_path = 'checkpoint_02.pt'):\n",
    "\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"assignment02.ipynb\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project = 'csye7374_ass02', \n",
    "        name = f\"lr{lr}_bs{batch_size}_embd{cfg['n_embd']}_L{cfg['n_layer']}\",\n",
    "        config = {**cfg, 'lr': lr, 'batch_size': batch_size,\n",
    "                'block_size': block_size, 'max_steps': max_steps}\n",
    "    )\n",
    "    wandb.watch(model, log = 'gradients', log_freq=100)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = 0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = n_epochs * max_steps)\n",
    "\n",
    "    history = {'loss': [], 'perplexity': []}\n",
    "    global_step = 0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        ds = GPTDataset(data_dir, block_size = block_size, shuffle = True)\n",
    "        loader = DataLoader(ds, batch_size = batch_size)\n",
    "\n",
    "        epoch_loss, steps = 0.0, 0\n",
    "        pbar = tqdm(loader, desc=f'Epoch {epoch}/{n_epochs}', total = max_steps, leave = True)\n",
    "\n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            _, loss = model(xb, yb) # Forward\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # recorded each step to Wandb\n",
    "            wandb.log({\n",
    "                'train/loss': loss.item(),\n",
    "                'train/lr':   scheduler.get_last_lr()[0],\n",
    "            }, step=global_step)\n",
    "\n",
    "            pbar.set_postfix(loss = f'{loss.item():.4f}')\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "\n",
    "        # record avg loss & perplexity at the end of each epoch\n",
    "        avg_loss = epoch_loss / steps\n",
    "        ppl = math.exp(avg_loss)\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['perplexity'].append(ppl)\n",
    "\n",
    "        wandb.log({\n",
    "            'epoch/avg_loss':   avg_loss,\n",
    "            'epoch/perplexity': ppl,\n",
    "            'epoch':            epoch,\n",
    "        }, step = global_step)\n",
    "\n",
    "        print(f' Avg Loss: {avg_loss:.4f} | Perplexity: {ppl:.2f}')\n",
    "\n",
    "    torch.save({'model_state': model.state_dict(),\n",
    "                'config': cfg,\n",
    "                'history': history}, ckpt_path)\n",
    "    print(f'Checkpoint saved → {ckpt_path}')\n",
    "\n",
    "    wandb.finish()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23af4a-8bfa-48c5-a7ec-c1031b9490d1",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ca7969-8870-4dd3-a1a7-1e7cc605bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /Users/zhenting/.netrc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 14,478,592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc-tingkuo216\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/zhenting/7374_LLM/Assignment_02/wandb/run-20260217_164234-ejhwlmzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/c-tingkuo216/csye7374_ass02/runs/ejhwlmzu' target=\"_blank\">lr0.0003_bs16_embd256_L2</a></strong> to <a href='https://wandb.ai/c-tingkuo216/csye7374_ass02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/c-tingkuo216/csye7374_ass02' target=\"_blank\">https://wandb.ai/c-tingkuo216/csye7374_ass02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/c-tingkuo216/csye7374_ass02/runs/ejhwlmzu' target=\"_blank\">https://wandb.ai/c-tingkuo216/csye7374_ass02/runs/ejhwlmzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████████▉| 1999/2000 [06:41<00:00,  4.98it/s, loss=5.6892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 6.4327 | Perplexity: 621.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████████▉| 1999/2000 [07:00<00:00,  4.76it/s, loss=5.4736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.6033 | Perplexity: 271.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████████▉| 1999/2000 [07:41<00:00,  4.33it/s, loss=5.5076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.4987 | Perplexity: 244.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████████▉| 1999/2000 [08:15<00:00,  4.03it/s, loss=5.3324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.3269 | Perplexity: 205.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████████▉| 1999/2000 [08:18<00:00,  4.01it/s, loss=5.2142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.2322 | Perplexity: 187.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████████▉| 1999/2000 [08:44<00:00,  3.81it/s, loss=5.3709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.0961 | Perplexity: 163.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████████▉| 1999/2000 [08:44<00:00,  3.81it/s, loss=4.7912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 4.9688 | Perplexity: 143.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████████▉| 1999/2000 [09:15<00:00,  3.60it/s, loss=5.1794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.0889 | Perplexity: 162.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████████▉| 1999/2000 [08:25<00:00,  3.95it/s, loss=4.8208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 4.9400 | Perplexity: 139.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|█████████████▉| 1999/2000 [11:33<00:00,  2.88it/s, loss=5.0834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 4.9871 | Perplexity: 146.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|████████████▉| 1999/2000 [12:57<00:00,  2.57it/s, loss=5.1827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 4.9428 | Perplexity: 140.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|████████████▉| 1999/2000 [12:51<00:00,  2.59it/s, loss=5.0764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.0967 | Perplexity: 163.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|████████████▉| 1999/2000 [12:50<00:00,  2.59it/s, loss=5.0924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 4.9493 | Perplexity: 141.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|████████████▉| 1999/2000 [12:53<00:00,  2.58it/s, loss=4.9535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 4.9128 | Perplexity: 136.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|████████████▉| 1999/2000 [15:17<00:00,  2.18it/s, loss=5.0880]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg Loss: 5.0529 | Perplexity: 156.48\n",
      "Checkpoint saved → checkpoint_02.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>epoch/avg_loss</td><td>█▄▄▃▂▂▁▂▁▁▁▂▁▁▂</td></tr><tr><td>epoch/perplexity</td><td>█▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▅▄▄▄▃▄▅▄▄▃▂▁▂▃▂▂▂▃▄▃▃▃▃▂▂▃▁▂▃▂▂▃▂▂▃▂▂</td></tr><tr><td>train/lr</td><td>████████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>epoch/avg_loss</td><td>5.05292</td></tr><tr><td>epoch/perplexity</td><td>156.4792</td></tr><tr><td>train/loss</td><td>5.08804</td></tr><tr><td>train/lr</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr0.0003_bs16_embd256_L2</strong> at: <a href='https://wandb.ai/c-tingkuo216/csye7374_ass02/runs/ejhwlmzu' target=\"_blank\">https://wandb.ai/c-tingkuo216/csye7374_ass02/runs/ejhwlmzu</a><br> View project at: <a href='https://wandb.ai/c-tingkuo216/csye7374_ass02' target=\"_blank\">https://wandb.ai/c-tingkuo216/csye7374_ass02</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260217_164234-ejhwlmzu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_DIR = '/Users/zhenting/7374_LLM/Assignment_01/tokenized_data'\n",
    "BLOCK_SIZE = 128\n",
    "VOCAB_SIZE = 50257 # GPT-2 tokenizer has a vocabulary of 50,257 tokens\n",
    "# At each position, the model outputs 50,257 logit scores — one for each token in the vocabulary\n",
    "# During training, these logits are passed to cross-entropy loss\n",
    "# During inference, a token is sampled or selected from these scores\n",
    "\n",
    "cfg = dict(vocab_size = VOCAB_SIZE, n_embd = 256, n_head = 4,\n",
    "           n_layer = 2, block_size = BLOCK_SIZE)\n",
    "\n",
    "model = MiniGPT(**cfg).to(DEVICE)\n",
    "print(f'Parameters: {model.num_params():,}')\n",
    "\n",
    "history = train(\n",
    "    model = model,\n",
    "    cfg = cfg,\n",
    "    data_dir = DATA_DIR,\n",
    "    block_size = BLOCK_SIZE,\n",
    "    batch_size = 16,\n",
    "    lr = 3e-4,\n",
    "    n_epochs = 15,\n",
    "    max_steps = 2000,\n",
    "    ckpt_path = 'checkpoint_02.pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279c4d3-2725-4cc6-bd6b-bb7ca758e46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c17492-f310-4848-9c08-3979b94885dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a0ba45e-6330-45fc-8679-682653fbe3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded\n",
      "Final loss history: [6.432705923080444, 5.603321623563766, 5.498742325305939, 5.326873038768769, 5.232220749139786, 5.096064509391785, 4.968841388463974, 5.088884825706482, 4.939992747306824, 4.987110480308533, 4.942793701887131, 5.096694251298905, 4.949284025907517, 4.91279996418953, 5.052923093795776]\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load('checkpoint_02.pt', weights_only=False)\n",
    "restored = MiniGPT(**ckpt['config']).to(DEVICE)\n",
    "restored.load_state_dict(ckpt['model_state'])\n",
    "restored.eval()\n",
    "print('Checkpoint loaded')\n",
    "print('Final loss history:', ckpt['history']['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecaa25d-b92d-4d1e-82cb-5443695434f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d22ce9-016a-49c3-925a-2916714c7dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889f296-e100-495f-9903-bae57532f69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd643184-03a7-41de-9357-7a808b8e73e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "7374_LLM",
   "language": "python",
   "name": "7374_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
